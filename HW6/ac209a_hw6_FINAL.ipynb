{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science \n",
    "\n",
    "## Homework 6  AC 209 : PCA\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2021**<br/>\n",
    "**Instructors**: Pavlos Protopapas and Natesh Pillai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 {\n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left;\n",
       "    padding-left: 10px;\n",
       "    background-color: #63ACBE;\n",
       "    color: black;\n",
       "}\n",
       "h2 {\n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left;\n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE;\n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #f8b4ab;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "\tbackground-color: #ffd0d0;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #63ACBE;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc {\n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 {\n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left;\n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE;\n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left;\n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD;\n",
       "    color: black;\n",
       "}\n",
       "span.emph {\n",
       "\tcolor: #601A4A;\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2021-CS109A/master/\"\n",
    "    \"themes/static/css/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- **THIS IS AN INDIVIDUAL ASSIGNMENT.**\n",
    "\n",
    "\n",
    "- Collaboration on this homework IS NOT PERMITTED.\n",
    "\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "\n",
    "\n",
    "- **PLEASE NOTE:**\n",
    "\n",
    "  - There are no coding exercises in this assignment.\n",
    "  - All of your mathematical statements should be written out in markdown cells using $\\LaTeX$.\n",
    "  - **Be sure to show your work so we can see how you arrived at your solutions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class='exercise'> Question 1 [20 pts] </div>\n",
    "\n",
    "Suppose we want to conduct PCA on the model matrix $X \\in \\Re^{n√óp}$, where the columns have been suitably set to zero mean. In this question, we consider the squared reconstruction error:\n",
    "\n",
    "$$  \\parallel XQ- XQ_m \\parallel ^2 $$\n",
    "\n",
    "for a suitable set of eigenvectors forming the matrix $Q_m$, as discussed below. Suppose that we conduct eigendecomposition of $X^T X$ and obtain eigenvalues $\\lambda_1, \\ldots , \\lambda_p$ and principal components $Q$, i.e.\n",
    "\n",
    "$$ X^T X = Q \\Lambda Q ^T $$\n",
    "\n",
    "**1.1** Suppose that the matrix norm is simply the squared dot product, namely\n",
    "\n",
    "$$ \\parallel A \\parallel ^2 = A^T A $$\n",
    "\n",
    "Then, express the reconstruction error as a sum of matrix products.\n",
    "\n",
    "**1.2**  Simplify your result from 1.1 based on properties of the matrices $Q$.\n",
    "\n",
    "**1.3** Now let $Q_m$ be the matrix of the first $m < p$ eigenvectors, namely\n",
    "\n",
    "$$ Q_m = (q_1, \\ldots, q_m, 0, \\ldots, 0) \\in \\Re^{p \\times p} $$\n",
    "\n",
    "Thus, $X Q_m$ is the PCA projection of the data into the space spanned by the first $m$ principal components. Express the products $Q^T_m Q$ and $Q^T Q_m$, again using properties of the eigenbasis $q_1, \\ldots, q_p$.\n",
    "\n",
    "**1.4**  Use your results from 1.3 to finally fully simplify your expression from 1.2.\n",
    "\n",
    "**1.5** Note that the result you obtain should still be a matrix, i.e. this does not define a proper norm on the space of matrices (since the value should be a scalar). Consequently, the true matrix norm is actually the trace of the above result, namely\n",
    "\n",
    "$$ \\parallel A \\parallel ^2  = {\\rm trace} (A^T A) $$\n",
    "\n",
    "Use your result from 1.4 and this new definition to find a simple expression for the reconstruction error in terms of the eigenvalues.\n",
    "\n",
    "**1.6** Interpret your result from 1.5. In light of your results, does our procedure for PCA (selecting the $m$ substantially larger eigenvalues) make sense? Why or why not?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.1** Suppose that the matrix norm is simply the squared dot product, namely\n",
    "\n",
    "$$ \\parallel A \\parallel ^2 = A^T A $$\n",
    "\n",
    "Then, express the reconstruction error as a sum of matrix products.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Re-expressing the reconstruction error:**\n",
    "\n",
    "\n",
    "\n",
    "$$||XQ - XQ_m||^2$$\n",
    "\n",
    "Given: $$ ||A||^2 = A^{T} A$$\n",
    "\n",
    "$$||XQ - XQ_m||^2 = (XQ - XQ_m)^{T}(XQ - XQ_m)$$\n",
    "\n",
    "$$ = (Q^T X^T - Q_m^T X^T)(XQ - XQ_m)$$\n",
    "\n",
    "$$ = Q^{T} X^{T} X Q - Q^{T} X^{T} X Q_m - Q_m^{T} X^{T} X Q_m + Q_m^{T} X^{T} X Q_m$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.2**  Simplify your result from 1.1 based on properties of the matrices $Q$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I know that $X^{T} X = Q \\Lambda Q^{T}$\n",
    "\n",
    "$$ = Q^{T} Q \\Lambda Q^{T} Q - Q^{T} Q \\Lambda Q^{T} Q_m - Q_m^{T} Q \\Lambda Q^{T} Q_m + Q_m^{T} Q \\Lambda Q^{T} Q_m$$\n",
    "\n",
    "$Q$ has p columns where each column is an eigenvector. $Q^{T} $ has p rows, where each row is an eigenvetor. \n",
    "\n",
    "I know that $Q^{T} Q  = I$ because of **orthonormality**. This means that the dot product of eigenvectors has to be 0. Where $ q_i^{T}q_j = \\delta_{ij}  $, meaning:\n",
    "\n",
    "* Due to orthonormality we have $q_i^{T}q_i = 1$, where $i = i$\n",
    "* and due to orthogonality we have  $q_i^{T}q_j = 0$, where $i \\neq  j$\n",
    "\n",
    "This means that the whole diagonal of the result of the multiplication $Q^{T} Q $ will be populated with values of 1. \n",
    "\n",
    "\n",
    "\n",
    "$$ Q^{T} Q  = \\begin{bmatrix} \n",
    "q_i^{T}q_i = 1  & q_i^{T}q_j = 0 & q_i^{T}q_j = 0 & 0 & 0 & 0 & ... & 0  \\\\ \n",
    "q_i^{T}q_j = 0 & q_i^{T}q_i = 1 & q_i^{T}q_j = 0  & 0 & 0 & 0 & ... & 0 \\\\\n",
    "q_i^{T}q_j = 0 & q_i^{T}q_j = 0 & q_i^{T}q_i = 1 & 0 & 0 & 0 & ... & 0 \\\\\n",
    "... \\\\ \n",
    "... \\\\ \n",
    "0 & 0  & 0 & 0  & ... & q_i^{T}q_i = 1 & q_i^{T}q_j = 0 &  q_i^{T}q_j = 0\\\\ \n",
    "0 & 0  & 0 & 0  & ... & q_i^{T}q_j = 0& q_i^{T}q_i = 1 &  q_i^{T}q_j = 0\\\\ \n",
    "0 & 0  & 0 & 0  & ... & 0 & q_i^{T}q_j = 0 &  q_i^{T}q_i = 1\\\\ \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$ Q^{T} Q  = \\begin{bmatrix} \n",
    "1  & 0 &  0 & 0 & 0 & 0 & ... & 0  \\\\ \n",
    "0 & 1 & 0  & 0 & 0 & 0 & ... & 0 \\\\\n",
    "0 & 0 & 1  & 0 & 0 & 0 & ... & 0 \\\\\n",
    "... \\\\ \n",
    "... \\\\ \n",
    "0 & 0 & 0 & 0  & ... & 1&  &  0\\\\ \n",
    "0 & 0  & 0 & 0  & ... & 0& 1 &  0\\\\ \n",
    "0 & 0  & 0 & 0  & ... & 0 &  0 &   1\\\\ \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Which is an identity matrix, $Q^{T} Q  = I$ of shape $p x p$ because also $Q$ and $Q^{T}$ are $ p x p$.\n",
    "\n",
    "\n",
    "\n",
    "$$ = I \\Lambda I - I \\Lambda Q^{T} Q_m - Q_m^{T} Q \\Lambda Q^{T} Q_m + Q_m^{T} Q \\Lambda Q^{T} Q_m$$\n",
    "\n",
    "$$ = \\Lambda  - \\Lambda Q^{T} Q_m - Q_m^{T} Q \\Lambda Q^{T} Q_m + Q_m^{T} Q \\Lambda Q^{T} Q_m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.3** Now let $Q_m$ be the matrix of the first $m < p$ eigenvectors, namely\n",
    "\n",
    "$$ Q_m = (q_1, \\ldots, q_m, 0, \\ldots, 0) \\in \\Re^{p \\times p} $$\n",
    "\n",
    "Thus, $X Q_m$ is the PCA projection of the data into the space spanned by the first $m$ principal components. Express the products $Q^T_m Q$ and $Q^T Q_m$, again using properties of the eigenbasis $q_1, \\ldots, q_p$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q_m$ has p columns where each column up to column $m$ is an eigenvector, from column $m+1$ to $p$ each column is populated with 0s. $Q_m^{T} $ has $p$ rows, where each row is an eigenvetor up to row $m$, from row $m+1$ to row $p$ each row is populated with 0s. \n",
    "\n",
    "\n",
    "\n",
    "Now $Q_m$ is also a $p x p$ matrix, but I have less eigenvectors. Meaning that I select m eigenvectors for my PCA analysis that can satisfy me enough to describe the variance of my original matrix X. Hence, I will not use all p eigenvectors. In this case, consequently, the $Q_m$ matrix has $m$ eigenvectors, one eigenvector each column, hence $m$ populated columns. The remaining columns up to $p$ are populated with $0$. \n",
    "\n",
    "Consequently, my multiplication between $Q_m^T Q$ will also be an identity matrix, but a truncated identity matrix, where on the diagonal I will have $m$ amount of ones, a one for each column of the $m$ columns. In the remainder, there will be $0$ values on the diagonals up to column $p$. \n",
    "\n",
    "Let's say, for sake of representation, that $m = p-2$. A rough representation will be:\n",
    "\n",
    "\n",
    "\n",
    "$$ Q_m^{T} Q  = \\begin{bmatrix} \n",
    "q_i^{T}q_i = 1  & q_i^{T}q_j = 0 & q_i^{T}q_j = 0 & 0 & 0 & 0 & ... & 0  \\\\ \n",
    "q_i^{T}q_j = 0 & q_i^{T}q_i = 1 & q_i^{T}q_j = 0  & 0 & 0 & 0 & ... & 0 \\\\\n",
    "q_i^{T}q_j = 0 & q_i^{T}q_j = 0 & q_i^{T}q_i = 1 & 0 & 0 & 0 & ... & 0 \\\\\n",
    "... \\\\ \n",
    "... \\\\ \n",
    "0 & 0  & 0 & 0  & ... & q_m^{T}q_m = 1 & q_i^{T}q_j = 0 &  q_i^{T}q_j = 0\\\\ \n",
    "0 & 0  & 0 & 0  & ... & 0 * q_j = 0 & 0*q_i = 0 &  q_i^{T}q_j = 0\\\\ \n",
    "0 & 0  & 0 & 0  & ... & 0 & 0* q_j = 0 &  0*q_i = 0\\\\ \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$ Q_m^{T} Q  = \\begin{bmatrix} \n",
    "1  & 0 &  0 & 0 & 0 & 0 & ... & 0  \\\\ \n",
    "0 & 1 & 0  & 0 & 0 & 0 & ... & 0 \\\\\n",
    "0 & 0 & 1  & 0 & 0 & 0 & ... & 0 \\\\\n",
    "... \\\\ \n",
    "... \\\\ \n",
    "0 & 0 & 0 & 0  & ... & 1&  &  0\\\\ \n",
    "0 & 0 & 0 & 0  & ... & 0& 0 &  0\\\\ \n",
    "0 & 0 & 0 & 0  & ... & 0 &  0 & 0\\\\ \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Hence, $Q_m^{T} Q  = Q^{T} Q_m = I_m $ where $I_m$ is a 'truncated' Identity matrix with 1 value up to column $m$, and 0 values on the diagonal up to column $p$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.4**  Use your results from 1.3 to finally fully simplify your expression from 1.2.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said above: $Q_m^{T} Q  = Q^{T} Q_m = I_m $ where $I_m$ is a 'truncated' Identity matrix with 1 value up to column $m$, and 0 values on the diagonal up to column $p$.\n",
    "\n",
    "Consequently, I can continue what we left above:\n",
    "\n",
    "$$||XQ - XQ_m||^2 = \\Lambda  - \\Lambda Q^{T} Q_m - Q_m^{T} Q \\Lambda Q^{T} Q_m + Q_m^{T} Q \\Lambda Q^{T} Q_m$$\n",
    "\n",
    "$$ = \\Lambda  - \\Lambda I_m - I_m \\Lambda + I_m \\Lambda I_m$$\n",
    "\n",
    "We can see that $I_m \\Lambda I_m = \\Lambda I_m = I_m \\Lambda $, hence:\n",
    "\n",
    "$$ = \\Lambda  - \\Lambda I_m $$\n",
    "\n",
    "Let's say, for sake of representation, that $m = p-2$. Hence:\n",
    "\n",
    "\n",
    "$$ \\Lambda = \\begin{bmatrix} \n",
    "\\lambda_1  & 0 &  0 & 0 & 0 & 0 & ... & 0  \\\\ \n",
    "0 & \\lambda_1  & 0  & 0 & 0 & 0 & ... & 0 \\\\\n",
    "0 & 0 & \\lambda_3  & 0 & 0 & 0 & ... & 0 \\\\\n",
    "... \\\\ \n",
    "... \\\\ \n",
    "0 & 0 & 0 & 0  & ... & \\lambda_{p-2} &  &  0\\\\ \n",
    "0 & 0 & 0 & 0  & ... & 0& \\lambda_{p-1}  &  0\\\\ \n",
    "0 & 0 & 0 & 0  & ... & 0 &  0 & \\lambda_{p} \\\\ \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$ \\Lambda I_m = \\begin{bmatrix} \n",
    "\\lambda_1  & 0 &  0 & 0 & 0 & 0 & ... & 0  \\\\ \n",
    "0 & \\lambda_1  & 0  & 0 & 0 & 0 & ... & 0 \\\\\n",
    "0 & 0 & \\lambda_3  & 0 & 0 & 0 & ... & 0 \\\\\n",
    "... \\\\ \n",
    "... \\\\ \n",
    "0 & 0 & 0 & 0  & ... & \\lambda_{m} &  &  0\\\\ \n",
    "0 & 0 & 0 & 0  & ... & 0& 0  &  0\\\\ \n",
    "0 & 0 & 0 & 0  & ... & 0 &  0 & 0 \\\\ \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$ \\Lambda - \\Lambda I_m  = \\begin{bmatrix} \n",
    "\\lambda_1  & 0 &  0 & 0 & 0 & 0 & ... & 0  \\\\ \n",
    "0 & \\lambda_1  & 0  & 0 & 0 & 0 & ... & 0 \\\\\n",
    "0 & 0 & \\lambda_3  & 0 & 0 & 0 & ... & 0 \\\\\n",
    "... \\\\ \n",
    "... \\\\ \n",
    "0 & 0 & 0 & 0  & ... & \\lambda_{p-2} &  &  0\\\\ \n",
    "0 & 0 & 0 & 0  & ... & 0& \\lambda_{p-1}  &  0\\\\ \n",
    "0 & 0 & 0 & 0  & ... & 0 &  0 & \\lambda_{p} \\\\ \n",
    "\\end{bmatrix} -  \\begin{bmatrix} \n",
    "\\lambda_1  & 0 &  0 & 0 & 0 & 0 & ... & 0  \\\\ \n",
    "0 & \\lambda_1  & 0  & 0 & 0 & 0 & ... & 0 \\\\\n",
    "0 & 0 & \\lambda_3  & 0 & 0 & 0 & ... & 0 \\\\\n",
    "... \\\\ \n",
    "... \\\\ \n",
    "0 & 0 & 0 & 0  & ... & \\lambda_{m} &  &  0\\\\ \n",
    "0 & 0 & 0 & 0  & ... & 0& 0  &  0\\\\ \n",
    "0 & 0 & 0 & 0  & ... & 0 &  0 & 0 \\\\ \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Which will look like this: \n",
    "\n",
    "\n",
    "$$ \\Lambda - \\Lambda I_m  = \\begin{bmatrix} \n",
    "0 & 0 &  0 & 0 & 0 & 0 & ... & 0  \\\\ \n",
    "0 & 0  & 0  & 0 & 0 & 0 & ... & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & ... & 0 \\\\\n",
    "... \\\\ \n",
    "... \\\\ \n",
    "0 & 0 & 0 & 0  & ... & 0 &  &  0\\\\ \n",
    "0 & 0 & 0 & 0  & ... & 0& \\lambda_{p-1}  &  0\\\\ \n",
    "0 & 0 & 0 & 0  & ... & 0 &  0 & \\lambda_{p} \\\\ \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Finally:\n",
    "\n",
    "$$||XQ - XQ_m||^2 = \\Lambda  - \\Lambda I_m $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.5** Note that the result you obtain should still be a matrix, i.e. this does not define a proper norm on the space of matrices (since the value should be a scalar). Consequently, the true matrix norm is actually the trace of the above result, namely\n",
    "\n",
    "$$ \\parallel A \\parallel ^2  = {\\rm trace} (A^T A) $$\n",
    "\n",
    "Use your result from 1.4 and this new definition to find a simple expression for the reconstruction error in terms of the eigenvalues.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$||XQ - XQ_m||^2 = trace(\\Lambda  - \\Lambda I_m) $$\n",
    "\n",
    "The trace of $(\\Lambda  - \\Lambda I_m)$ is the summation of the terms on the diagonal. Which is in this case:\n",
    "\n",
    "$$\\sum_{i = m+1}^{p} \\lambda_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.6** Interpret your result from 1.5. In light of your results, does our procedure for PCA (selecting the $m$ substantially larger eigenvalues) make sense? Why or why not?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The reconstruction error $||XQ - XQ_m||^2 = \\sum_{i = m+1}^{p} \\lambda_i$ is the sum of the eigenvalues that were not used and were not take into consideration for my reconstruction $\\sum_{i = 1}^{m} \\lambda_i$. This is because I am selecting the first $m$ eigenvalues for my reconstruction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Eigenvalues on the diagonal of $\\Lambda$ are ordered on the diagonal from the largest values to the smallest, and usually they decrease very fast. In $\\Lambda$ the first handful of eigenvalues, hence the first handful of principal components, usually can explain the most of the variance for the original matrix $X$. Consequently, in a usual case where first first few principal components already explain most of the variance (let's say aroung 90%), it means that my reconstruction error $\\sum_{i = m+1}^{p} \\lambda_i$ in that case, will be sort of negligible. This might not always be the case tho, as my first PC might also not be that big, i.e. 40%. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Consequently, out of this proof, I can only generally say that the re-construction error\n",
    "\n",
    "$$\\sum_{i = m+1}^{p} \\lambda_i$$\n",
    "\n",
    "is the amount of variance I didn't include.\n",
    "\n",
    "Meaning, that the error $\\sum_{i = m+1}^{p} \\lambda_i$ is inversely proportional to the n. of components that i choose to include in my reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
